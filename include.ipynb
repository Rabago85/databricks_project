{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95e0f73-2cda-4b3e-9e54-e4c6c423a8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Paths and Modules to Include\n",
    "##Purpose:\n",
    "- This notebook contains paths to directories containing the orginal csv's on the DBFS, as well as directories that will be used to contain the parquet files after ingestion and transformation.\n",
    "- Also included:\n",
    "  - A class csv_reader that loads each csv file into a dataframe that is generalized for both plan and rate files.  This was necessary due to changing schemas and non-quotes in one year's headers that made simple ingestion from a directory difficult.\n",
    "  - A class SchemaDefiner that allows me to define a schema in python as I would a Dataclass, rather than using StructType[StructField].  This allows for a simple API to define a shema that resembles how a schema is defined in scala to use with DataSets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52808b6b-968e-4ccd-8821-77a15ae5ba86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField\n",
    "\n",
    "# raw_path = spark.sql(f'DESCRIBE EXTERNAL LOCATION landing').select('url').collect()[0].url\n",
    "# raw_rates_path = os.path.join(raw_path, 'raw_rates')\n",
    "# raw_plans_path = os.path.join(raw_path, 'raw_plans')\n",
    "\n",
    "# processed_path = spark.sql(f'DESCRIBE EXTERNAL LOCATION bronze').select('url').collect()[0].url\n",
    "# processed_rates_path = os.path.join(processed_path, \"rates\")\n",
    "# processed_plans_path = os.path.join(processed_path, \"plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e63332-eefa-40c3-b373-72eec8e166d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class csv_reader:\n",
    "    def __init__(self, cols_dict, schema):\n",
    "        self.cols_dict = cols_dict\n",
    "        self.schema = schema.schema\n",
    "        self.df = None\n",
    "        \n",
    "\n",
    "    def load_csv(self, pth):\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(pth)\n",
    "        self.df = df\n",
    "        return self\n",
    "\n",
    "    def check_year_type(self):\n",
    "        if dict(self.df.dtypes)[\"BusinessYear\"] != \"int\":\n",
    "            self.df = self.df.withColumn(\n",
    "                \"BusinessYear\", F.col(\"BusinessYear\").cast(IntegerType())\n",
    "            ).filter(F.col(\"BusinessYear\").isNotNull())\n",
    "        return self\n",
    "\n",
    "    def rename_cols(self):\n",
    "        self.df = self.df.withColumnsRenamed(self.cols_dict)\n",
    "        return self\n",
    "\n",
    "    def select_cols(self):\n",
    "        self.df = self.df.select(*self.cols_dict.values())\n",
    "        return self\n",
    "\n",
    "    def validate_schema(self):\n",
    "        if self.schema != self.df.schema:\n",
    "            raise Exception(\"Dataframe Schema different from what was expected.\")\n",
    "        return self\n",
    "            \n",
    "    def csv_to_df(self, pth):\n",
    "        self.load_csv(pth).check_year_type().rename_cols().select_cols().validate_schema()\n",
    "        return self.df\n",
    "\n",
    "class SchemaDefiner:\n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "        \n",
    "    @classmethod    \n",
    "    def get_pyspark_schema(cls, data_class_obj):\n",
    "        schema_dict = data_class_obj.__annotations__\n",
    "        struct_fields = []\n",
    "        for col, col_type in schema_dict.items():\n",
    "            struct_fields.append(StructField(col, col_type()))\n",
    "        return cls(StructType(struct_fields))\n",
    "\n",
    "    def __call__(self):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff2da54-10b1-4055-85e2-cff6941f65c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "print('End')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "include",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}